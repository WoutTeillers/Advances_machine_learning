{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c924673f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.lstm import LSTM\n",
    "from src.trainer import Trainer, EarlyStopping\n",
    "from data.data_creation import get_trajectories, plot_trajectories, plot_boxplots\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ac3a78",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c0de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/trajectories.npy'):\n",
    "    x, y, vx, vy, t = get_trajectories()\n",
    "    np.save('data/trajectories.npy', [x, y, vx, vy, t])\n",
    "else:\n",
    "    x, y, vx, vy, t = np.load('data/trajectories.npy', allow_pickle=True)\n",
    "\n",
    "# Stack into a (num_samples, 12) array\n",
    "data = np.vstack((x, y, vx, vy)).T\n",
    "print(f\"Data shape: {data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd719176",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.85\n",
    "train_end = int(train_test_split * len(data))\n",
    "train_data = data[:train_end]\n",
    "test_data = data[train_end:]\n",
    "print(f\"Train: {train_data.shape}, Test: {test_data.shape}\")\n",
    "\n",
    "# --- Normalize Data ---\n",
    "scaler = RobustScaler()  # or MinMaxScaler()\n",
    "scaler.fit(train_data)\n",
    "train_data_scaled = scaler.transform(train_data)\n",
    "test_data_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Save scaler for later use\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "with open(\"data/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7102ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xy(data, lag=50):\n",
    "    X = torch.tensor(data[:-lag], dtype=torch.float32)\n",
    "    y = torch.tensor(data[lag:], dtype=torch.float32)\n",
    "    return X, y\n",
    "\n",
    "lag = 50\n",
    "X_train, y_train = generate_xy(train_data_scaled, lag)\n",
    "X_test, y_test = generate_xy(test_data_scaled, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120b5cb9",
   "metadata": {},
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data):\n",
    "    Q1 = np.percentile(data, 25, axis=0)\n",
    "    Q3 = np.percentile(data, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return np.clip(data, lower, upper)\n",
    "\n",
    "data_clean = remove_outliers(data)\n",
    "plot_boxplots(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d5a8c",
   "metadata": {},
   "source": [
    "### Train stateful models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44722515",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [256, 512, 1028]  # adjust as needed\n",
    "num_epochs = 100\n",
    "\n",
    "os.makedirs(\"stateful_models\", exist_ok=True)\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\n--- Training LSTM with hidden size {hidden_size} ---\")\n",
    "    model = LSTM(input_size=12, hidden_size=hidden_size, output_size=12, initializer_method='xavier').to(device)\n",
    "    early_stopping = EarlyStopping(patience=1)\n",
    "    trainer = Trainer(model, learning_rate=0.001, early_stopping=early_stopping)\n",
    "    \n",
    "    trainer.train(X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device), epochs=num_epochs)\n",
    "    \n",
    "    # Save model and training history\n",
    "    model_path = f\"stateful_models/lstm_stateful_h{hidden_size}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    np.save(f\"stateful_models/train_losses_h{hidden_size}.npy\", np.array(trainer.train_losses))\n",
    "    np.save(f\"stateful_models/val_losses_h{hidden_size}.npy\", np.array(trainer.val_losses))\n",
    "    \n",
    "    print(f\"Saved model and losses for hidden size {hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879868b6",
   "metadata": {},
   "source": [
    "### Training/validation loss plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d82068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256  # choose which model to plot\n",
    "train_losses = np.load(f\"stateful_models/train_losses_h{hidden_size}.npy\", allow_pickle=True)\n",
    "val_losses = np.load(f\"stateful_models/val_losses_h{hidden_size}.npy\", allow_pickle=True)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_losses, color='black', label='Training Loss')\n",
    "plt.plot(val_losses, color='blue', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'LSTM Loss - Hidden size {hidden_size}')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217bfe3a",
   "metadata": {},
   "source": [
    "### Iterative predictions vs ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialization_steps = 1000\n",
    "steps = 2000\n",
    "\n",
    "model.load_state_dict(torch.load(f\"stateful_models/lstm_stateful_h{hidden_size}.pt\"))\n",
    "model.eval()\n",
    "\n",
    "output = model.generate_timeseries(X_test[:initialization_steps].to(device), steps=steps)\n",
    "true = y_test[initialization_steps:steps]\n",
    "\n",
    "# Convert back to original scale\n",
    "true_np = scaler.inverse_transform(true.detach().numpy())\n",
    "output_np = scaler.inverse_transform(output.detach().numpy())\n",
    "\n",
    "plot_trajectories(true_np, output_np)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4565ec0",
   "metadata": {},
   "source": [
    "### Distribution of predicted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "for i in range(12):\n",
    "    plt.plot(output_np[:, i], alpha=0.6, label=f\"Feature {i+1}\")\n",
    "    plt.fill_between(range(len(output_np)), output_np[:, i], alpha=0.1)\n",
    "plt.title(\"Predicted Feature Distribution\")\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"Feature value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
